# MCP OpenAI Image Generation Server

[![npm version](https://img.shields.io/npm/v/@lupinlin1/imagegen-mcp)](https://www.npmjs.com/package/@lupinlin1/imagegen-mcp)

> ğŸš€ **é›¶å®‰è£…é…ç½®ï¼** ç›´æ¥åœ¨MCPå®¢æˆ·ç«¯ä¸­ä½¿ç”¨ï¼Œæ— éœ€ä»»ä½•é¢„å®‰è£…æ­¥éª¤
>
> ```json
> {
>   "mcpServers": {
>     "imagegen-mcp": {
>       "command": "npx",
>       "args": ["@lupinlin1/imagegen-mcp", "--models", "dall-e-3"],
>       "env": { "OPENAI_API_KEY": "your_api_key" }
>     }
>   }
> }
> ```

This project provides a server implementation based on the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) that acts as a wrapper around OpenAI's Image Generation and Editing APIs (see [OpenAI documentation](https://platform.openai.com/docs/api-reference/images)).

## Features

*   Exposes OpenAI image generation capabilities through MCP tools.
*   Supports `text-to-image` generation using models like DALL-E 2, DALL-E 3, and gpt-image-1 (if available/enabled).
*   Supports `image-to-image` editing using DALL-E 2 and gpt-image-1 (if available/enabled).
*   Configurable via environment variables and command-line arguments.
*   Handles various parameters like size, quality, style, format, etc.
*   Saves generated/edited images to temporary files and returns the path along with the base64 data.

Here's an example of generating an image directly in Cursor using the `text-to-image` tool integrated via MCP:

<div align="center">
  <img src="https://raw.githubusercontent.com/spartanz51/imagegen-mcp/refs/heads/main/cursor.gif" alt="Example usage in Cursor" width="600"/>
</div>

## ğŸš€ å®‰è£…æ–¹å¼

### ğŸ¯ é›¶å®‰è£…é…ç½® (æ¨è)

**æ–¹å¼1: NPXè‡ªåŠ¨ä¸‹è½½** (éœ€è¦NPMå‘å¸ƒ)
```bash
npm install -g @lupinlin1/imagegen-mcp
```

**æ–¹å¼2: GitHubè¿œç¨‹æ‰§è¡Œ** (ç«‹å³å¯ç”¨)
```bash
# ä¸€è¡Œå‘½ä»¤å®‰è£…è„šæœ¬
curl -fsSL https://raw.githubusercontent.com/LupinLin1/imagegen-mcp/main/scripts/install.sh | bash
```

**æ–¹å¼3: æœ¬åœ°è„šæœ¬** (å¼€å‘è€…å‹å¥½)
```bash
git clone https://github.com/LupinLin1/imagegen-mcp.git
cd imagegen-mcp
npm install && npm run build
```

### ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

| æ–¹å¼ | å®‰è£…æ­¥éª¤ | ç½‘ç»œä¾èµ– | å¯åŠ¨é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|----------|----------|
| NPXè‡ªåŠ¨ä¸‹è½½ | 0æ­¥ | é¦–æ¬¡éœ€è¦ | å¿« | ç”Ÿäº§ç¯å¢ƒ |
| GitHubè¿œç¨‹ | 0æ­¥ | æ¯æ¬¡éœ€è¦ | ä¸­ç­‰ | å¿«é€Ÿè¯•ç”¨ |
| æœ¬åœ°è„šæœ¬ | 1æ­¥å…‹éš† | æ—  | æœ€å¿« | å¼€å‘æµ‹è¯• |

ğŸ“ **æ›´å¤šé…ç½®**: æŸ¥çœ‹ [`examples/mcp-configs/`](./examples/mcp-configs/) è·å–æ‰€æœ‰é…ç½®ç¤ºä¾‹

## Prerequisites

*   Node.js (v18 or later recommended)
*   npm or yarn
*   An OpenAI API key

## ğŸ¯ é›¶å®‰è£…é…ç½® (æ¨è)

**æ— éœ€ä»»ä½•é¢„å®‰è£…æ­¥éª¤ï¼**ç›´æ¥é…ç½®å³å¯ä½¿ç”¨ï¼š

### Cursor ç¼–è¾‘å™¨
```json
{
  "mcpServers": {
    "imagegen-mcp": {
      "command": "npx",
      "args": ["@lupinlin1/imagegen-mcp", "--models", "dall-e-3"],
      "env": {
        "OPENAI_API_KEY": "your_openai_api_key_here"
      }
    }
  }
}
```

### Claude Desktop
```json
{
  "mcpServers": {
    "imagegen-mcp": {
      "command": "npx",
      "args": ["@lupinlin1/imagegen-mcp"],
      "env": {
        "OPENAI_API_KEY": "your_openai_api_key_here"
      }
    }
  }
}
```

### ğŸ’¡ é›¶å®‰è£…åŸç†
- âœ… **é¦–æ¬¡è¿è¡Œ**: `npx` è‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜åŒ…
- âœ… **åç»­å¯åŠ¨**: ä½¿ç”¨ç¼“å­˜ï¼Œå¯åŠ¨å¿«é€Ÿ
- âœ… **è‡ªåŠ¨æ›´æ–°**: å§‹ç»ˆä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
- âœ… **æ— æ±¡æŸ“**: ä¸ä¼šå…¨å±€å®‰è£…ä»»ä½•åŒ…

ğŸ“ **æ›´å¤šé…ç½®ç¤ºä¾‹**: æŸ¥çœ‹ [`examples/mcp-configs/`](./examples/mcp-configs/) ç›®å½•

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repository-url>
    cd <repository-directory>
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    # or
    yarn install
    ```

3.  **Configure Environment Variables:**
    Create a `.env` file in the project root by copying the example:
    ```bash
    cp .env.example .env
    ```
    Edit the `.env` file and add your OpenAI API key:
    ```
    OPENAI_API_KEY=your_openai_api_key_here
    ```

## Building

To build the TypeScript code into JavaScript:
```bash
npm run build
# or
yarn build
```
This will compile the code into the `dist` directory.

## Running the Server

This section provides details on running the server locally after cloning and setup. For a quick start without cloning, see the [Quick Run with npx](#quick-run-with-npx) section.

**Using ts-node (for development):**
```bash
npx ts-node src/index.ts [options]
```

**Using the compiled code:**
```bash
node dist/index.js [options]
```

**Options:**

*   `--models <model1> <model2> ...`: Specify which OpenAI models the server should allow. If not provided, it defaults to allowing all models defined in `src/libs/openaiImageClient.ts` (currently gpt-image-1, dall-e-2, dall-e-3).
    *   Example using `npx` (also works for local runs): `... --models gpt-image-1 dall-e-3`
    *   Example after cloning: `node dist/index.js --models dall-e-3 dall-e-2`

The server will start and listen for MCP requests via standard input/output (using `StdioServerTransport`).

## MCP Tools

The server exposes the following MCP tools:

### `text-to-image`

Generates an image based on a text prompt.

**Parameters:**

*   `text` (string, required): The prompt to generate an image from.
*   `model` (enum, optional): The model to use (e.g., `gpt-image-1`, `dall-e-2`, `dall-e-3`). Defaults to the first allowed model.
*   `size` (enum, optional): Size of the generated image (e.g., `1024x1024`, `1792x1024`). Defaults to `1024x1024`. Check OpenAI documentation for model-specific size support.
*   `style` (enum, optional): Style of the image (`vivid` or `natural`). Only applicable to `dall-e-3`. Defaults to `vivid`.
*   `output_format` (enum, optional): Format (`png`, `jpeg`, `webp`). Defaults to `png`.
*   `output_compression` (number, optional): Compression level (0-100). Defaults to 100.
*   `moderation` (enum, optional): Moderation level (`low`, `auto`). Defaults to `low`.
*   `background` (enum, optional): Background (`transparent`, `opaque`, `auto`). Defaults to `auto`. `transparent` requires `output_format` to be `png` or `webp`.
*   `quality` (enum, optional): Quality (`standard`, `hd`, `auto`, ...). Defaults to `auto`. `hd` only applicable to `dall-e-3`.
*   `n` (number, optional): Number of images to generate. Defaults to 1. Note: `dall-e-3` only supports `n=1`.

**Returns:**

*   `content`: An array containing:
    *   A `text` object containing the path to the saved temporary image file (e.g., `/tmp/uuid.png`).

### `image-to-image`

Edits an existing image based on a text prompt and optional mask.

**Parameters:**

*   `images` (string, required): An array of *file paths* to local images.
*   `prompt` (string, required): A text description of the desired edits.
*   `mask` (string, optional): A *file path* of mask image (PNG). Transparent areas indicate where the image should be edited.
*   `model` (enum, optional): The model to use. Only `gpt-image-1` and `dall-e-2` are supported for editing. Defaults to the first allowed model.
*   `size` (enum, optional): Size of the generated image (e.g., `1024x1024`). Defaults to `1024x1024`. `dall-e-2` only supports `256x256`, `512x512`, `1024x1024`.
*   `output_format` (enum, optional): Format (`png`, `jpeg`, `webp`). Defaults to `png`.
*   `output_compression` (number, optional): Compression level (0-100). Defaults to 100.
*   `quality` (enum, optional): Quality (`standard`, `hd`, `auto`, ...). Defaults to `auto`.
*   `n` (number, optional): Number of images to generate. Defaults to 1.

**Returns:**

*   `content`: An array containing:
    *   A `text` object containing the path to the saved temporary image file (e.g., `/tmp/uuid.png`).

## Development

*   **Linting:** `npm run lint` or `yarn lint`
*   **Formatting:** `npm run format` or `yarn format` (if configured in `package.json`)

## Contributing

Pull Requests (PRs) are welcome! Please feel free to submit improvements or bug fixes. 